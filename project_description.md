# Camera-based Multimodal Emotion Detection and Reaction

Intelligent human-agent collaboration requires the software agent to read the current situation correctly. In the context of an instructive assistance system, the software agent must align its level of support to its userâ€™s capabilities. One assumption could be that a user that sounds or looks stressed or unconfident requires a higher level of support than a user who is calm and relaxed.


## Objective
The objective of this topic is to develop a microservice that detects the current emotion of a user based on his voice and facial expression from a video snippet. 

## Milestones
Project period: Middle of April - End of September

### 1. Identify the Problem
Read papers/resources about SER, FER and Multimodal emotion recognition

* Similar project: https://github.com/maelfabien/Multimodal-Emotion-Recognition
* IEMOCAP dataset: https://www.kaggle.com/datasets/samuelsamsudinng/iemocap-emotion-speech-database
* RAVDESS dataset: https://smartlaboratory.org/ravdess/
* LSTM + CNN model: https://arxiv.org/pdf/1804.05788v3.pdf
* BERT-based model: https://arxiv.org/pdf/2202.08974.pdf

### 2. Define the Approaches
* Modality based: textual, image, video(audio)
* Model based: handcrafted features(mfcc,chroma..) or DL
* How to combine the modalities/fusion 

### 3. Evaluate 


### 4. Inference on real-time camera


### 5.Demostration Setup


